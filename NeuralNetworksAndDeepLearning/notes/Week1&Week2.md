# 深度学习入门

## 1  摘要
这两周内容比较简单，主要是通过逻辑回归来引入，使用python实现，熟悉课程环境等等。都很简单，但是python确实比起matlab麻烦点，主要是一些语言的设计不如matlab直观。我把coursera hub上面的资源扒下来，用vs跑，也是有些小问题。

然后，比较重要的是逻辑回归的损失函数的设计原理。

## 2  逻辑回归
逻辑回归可以看作一个最简单的神经网络。原因如下。

<img src = 'images\2019-07-15_172203.jpg'/>

关于逻辑回归，一直以来的疑惑就是，损失函数引入的log，感觉没啥根据。其实如下。

<img src = 'images\2019-07-15_145403.jpg'/>

yhat就是我们的预测结果，为了将两种预测结果引入统一等式，所以使用了指数的形式。但问题在于，指数化的损失函数，图像太过复杂，有局部最优，所以在损失函数上又引入了log，因为log函数是严格的单调递增函数——即引入不影响原函数的性质。于是就有了最后的等式。

当然，这只是一个输出，多个输出呢？需要用到独立概率的计算公式。如下。

<img src = 'images\2019-07-15_145755.jpg'/>

最终变成我们熟悉的形式。log将连乘转化为连加。

## 3  其他
**numpy的广播机制**，这是一个很好用的语法，但是容易出现隐藏bug，解决方法是经常检查数据的size，使用assert是一个好习惯

**过拟合于学习率**

学习率的目的只需让cost尽可能小即可，但是在过拟合的情况下，结果不一定好。但这个标准是对的。

## 4  练习
1. python和numpy的入门
2. 读取数据，数据预处理
3. sigmoid，init，propagate
4. 梯度下降optimize
5. 预测predict
6. 整合上述函数model
7. learning rate 和overfit的讨论

由于才看到了coursera的荣誉准则，所以将github的仓库私有化了，不提供代码了。