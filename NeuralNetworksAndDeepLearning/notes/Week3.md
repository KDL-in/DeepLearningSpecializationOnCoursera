# 浅层神经网络

## 1  摘要
搭建浅层神经网络做平面点的分类，基本都已经做过，改成矩阵形式，主要还是对神经网络的推导以及一些问题的讨论。

- 为什么不使用线性函数作为激活函数？
- 为什么reLU有用？（未知）
- 为什么要使用随机初始化？
- 为什么初始化参数不能太大？

## 2  神经网络推导
神经网络的模型，基本原理见另外两篇笔记。

基本结果是这样的。

<img src = 'images\1563322892484.png'/>

主要是讨论反向传播所求的梯度。

下图是完整的公式（上标表示层数），dz只是J对z偏导的简写。看到红色的箭头，dz2和dW2求出来后，就能求dz1，dz1求出来后才能求dW1，dz1和dW1求出来后......换句话说，我们是运用链式法则来求对每一个w的偏导的，但不用每一层都重新从J开始偏导，而是用上一层的值推下一层的值。

<img src = 'images\Inked神经网络梯度下降矩阵实现_LI.jpg'/>

推导的过程如下。图的结构是这样的，上面是链式依赖，右边是对输出层（最后一层）的推导，左边是对前一层的推导。主要最终的任务是：dz[j]、dW[j] -> dz[j-1]，dW[j-1] -> dz[j-2]、dW[j-2] -> .....

<img src = 'images\img_20190716_172541.767.png'/>

## 3  一些问题的讨论
**为什么激活函数不使用线性函数**？

是这样，a（线性函数）x  b（线性函数）x c（线性函数）... = k（线性函数），神经网路中各个单元，各层之间的组合，最终会形成最后的输出函数，如果激活函数本身就是线性的，那么，最终组成的函数也是线性的——那就和逻辑回归没啥分别了，神经网络没有意义。

**为什么一定要随机初始化**？

回答这个问题，应该从反面回答，为什么将w和b初始化成相同的值不行。我们已经知道，每一层的单元都是由上一层的所有单元的连接贡献，如果w和b都一样，那么每一层的每个单元就都一样。这意味着，所有单元计算的都是同一个特征，那么就是逻辑回归，神经网络没有意义。

**为什么初始化w和b不能太大？**

假设用的是tanh函数，已知x>>0和x<<0的时候，函数都会趋近水平——意味着梯队下降会变得极其慢，所以w和b不能太大，而且，如果特征多，那么w和b还得变小，防止加权和太大。

## 4  练习
浅层神经网络，使用浅层神经网络做多分类

**一、数据集操作**

**二、简单逻辑回归**，用的是sklearn的库直接体现分类效果

**三、浅层神经网络模型**

*初始化*

1. 初始化各层神经元数量
2. 随机初始化参数

*模型搭建*

1. 向前传播 ->  A2, cache = {"Z1": Z1,"A1": A1, "Z2": Z2,"A2": A2} 主要目的是最终结果A2
2. 计算J，即cost
3. 向后传播 -> grads， 计算梯度
4. 梯度下降，更新参数

*整合模型*

1. 整合模型

隐藏层为4已经可以很好的解决平面点上的分类问题，按照Ng的代码搭建的模型用起来非常方便。