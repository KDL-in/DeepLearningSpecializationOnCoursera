# 深层神经网络

## 1  摘要
本周的主要任务，python实现深层神经网络模型，其实原理没变，只是不能再使用硬编码，层次之间要迭代，需要更好的递推性和更好的结构。所以，原理早就学过了，本周的任务其实就是模型实现的结构设计本身。

讨论了一些问题。

- 为什么深度学习表示很好？
- 深度学习与大脑类比好吗？

## 2  深度神经网络结构设计
我觉得这个具体还是看练习一的总结，其实就是向前传播和向后传播的完成。

<img src = 'images\backward&forward.jpg'/>

L_model_forward，多层向后传播函数的数学模型如下。

<img src = 'images\2019-07-17_152759.jpg'/>

L_model_backward，多层向后传播的数学模型如下。

<img src = 'images\2019-07-17_152742.jpg'/>

你会发现，神经网络其实是这样的，**分层函数模型 + 多层传播原理（主要是求导原理） + 实现结构设计**，前面两部分其实也很好理解，最后发现结构设计本身很麻烦，又回到了开发能力上。ps，分层模型参考机器学习笔记week4，多层传播模型参考机器学习week 5 和深度学习笔记week 2、week 3，结构设计参考本篇。、

## 3  超参数
机器学习原理，使得函数的参数都能自动找到。但是深度神经网络带来模型本身的参数，比如几层合适？每层神经元数量？学习效率？迭代次数？

这些才是正在麻烦的地方——所谓调参，你需要不断通过实验结果来调整参数，而神经网络模型本身就特别复杂，甚至环境变动，如cpu，gpu，甚至计算机系统结构变化这些都会变，所以叫炼丹不是吹的。

这些参数，叫做超参数，模型调参，也成了深度神经网络模型的大问题。

## 4  问题
**为什么深度学习很有效？**

> 首先最重要的是效率问题，只要能跑起来才能谈其他。效率问题得益于分层的结构，参考二叉树（电路理论），树高度只有logn，而如果是单层的话，节点数是2的n次方。
>
> 其次，以图像识别为例，其实查看隐藏层可以发现，每一层提取的特征越来越复杂，可以直观地理解，点、线、部分轮廓、轮廓......这很像人类的模型识别能力。

**深度学习的人工神经网络和人脑的神经网络类比如何？**

> 可以感觉到，人工神经网络确实是借鉴了很多人类神经科学的概念，很长时间以来我们都以此作为类比。但这会有些危险，人类单个神经元的全部原理，至今人类只知道一小部分，人脑到底是怎么识别的，是不是也存在梯度下降的类似能力，或者是有一套完全不一样的原理？这些都是未知的——可能它们压根不是一种东西。
>
> 其次，深度学习本身就是指隐藏层很多的人工神经网络，本质上就是一个多参数方程，但是现在环境下容易产生误解——即使是我这种计算机从业者也不例外。
>
> 所以结论是，现在还是不用这种类比比较合适。

## 5  练习一
一步步搭建深层网络模型，这个其实和之前差不多，只不过需要非常好的递推性，层次不再是硬编码，所以细节很多。

基本上是这样的，主要函数是两个，分别为向前传播中的L_model_forward——多层向前传播的模型，以及向后传播的L_model_backward——多层向后传播的模型。

麻烦的地方有两个，一个是多层传播模型必须要保持一致性——可以用循环实现。其次是，向前传播中，必须留下很多caches给向后传播使用。

initialize_parameters_deep -->parameters，初始化，包含所有W和b

**forward**

> **L_model_forward** -> AL, caches，向前传播，目的是计算最终激活值，caches中为（（A[L-1]，W[L]，Z[L]），Z[L]）x m

激活函数不同，所以使用到以下函数

> **linear_activation_forward** -> A, caches，计算A = g(Z)，caches值为（（A[L-1]，W[L]，Z[L]），Z[L]）

使用了以下函数

> linear_backward -> dA_prev, dW, db，计算（A[L-1]，W[L]，Z[L]）

**backward**

> **L_model_backward** -> grads，向后传播，目的是计算grad，其中包含（dA，dW，db）x m

激活函数不同，使用了以下函数

> **linear_activation_backward**-> dA_prev, dW, db，要算dA-1，必须算dZ，dZ根据激活函数不同，使用不同的求导，然后计算（dA-1，dW，db）

使用了以下函数

> **linear_backward**-> dA_prev, dW, db

## 6  练习二
整合模型，使用上次作业写的函数，对模型进行整合，很简单。

**L_layer_model**，开始梯度下降，每一次使用向前传播，向后传播，更新参数，输出结果

predict中再次调用L层向前传播其实并不耗时——这只是对几个数据跑一遍现成参数而已，真正耗时的是梯度下降时对L层前后传播的调用——每次下降所有数据都要跑一次。

