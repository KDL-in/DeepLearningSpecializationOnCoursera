# Word Embeddings&文本情感分析

## 摘要

本周围绕这Word Embeddings展开。讲了它是什么，有什么用；其次是提到了它相关的两种训练算法；最后应用到文本情感分类，词性相似推理，以及消除歧视问题上。

- Word Embeddings
- Sentiment Classification

我到现在还没理解为啥要叫这么不直观的名字，可能是我理解的不到位。

本周的留下了很多问题，理解起来不是那么直观，nlp的算法还是稍微复杂一些的。待之后深入。遗留问题如下：

- t-SNE降维算法
- 如何训练word embeddings，包括负采样，GloVe，Word2Vec几种算法
- 消除歧视的时候使用到的线性代数的投影公式。

## Word Embeddings是什么？

讲了半天，我实在是没能理解它究竟是什么，并且，看的过程中老是喜欢把它翻译成单词嵌入？这让我极其费解。

作业实现的时候，我总结了一下，其实WE（以下简称）就是Word的特征向量，但是它这些抽取出来的特征不是从它本身来的（CNN中训练得到的特征是从它本身，即图片本身得来的），而是从上下文得来的。

我感觉这是一件非常非常有意思的事情，这是一个新的特征抽取思路，可以解决上下文的问题。

其次，再次思考rnn的相关算法。cnn可以说是对空间特征的抽取，rnn则是对时间序列的抽取。之所以能收敛，前者，应该是因为图片空间上有规律，后者，则因为文本在时间上有规律。我有种感觉，这是一个接近深度学习本质的认知。

## 应用一：名词相似推理

### 问题

这是一个很有意思的问题。

> 假设，man对应king，那么请问women对应什么？很简单吧，queen。同理，man对应father，那么请问women对应什么？mothe！很简单吧。

可是仔细想想，这样一件简单的事情，计算机能做吗？（建立一个字典不算哈），这件事的关键之处在于教会计算机人类推理的能力，这几乎是不可能的。

比如，人是怎么做到的？仔细想想，其实我们也完全不清楚，我们是怎么做出这种推理的呢？首先我们得知道man是什么，再要知道king是什么，再要知道国王从历史上来看男的比较多这样的事，再联想起女应该对应什么……做过深度学习就知道，其实认知是最大的难题。

但是这里使用了一种非常有意思方式完成了这件事。

### Solution

核心的思想是：

> 用统计的方法量化一个word的上下文关系（通过大量统计得知一般来说这个词和其他词的上下文关系），两个词之间的关系可以用向量差来表示。于是，我们只需要让两对词之间的两个关系，尽可能相似。

 上文我们提到，WE就是这样一种量化单词向量的方法。于是，man和king向量的关系，我们可以用它们向量差来表示。woman向量需要和所有词都作差，找出一个关系，这个关系向量和上述关系向量最为接近。

不知道你晕了没有，可能我描述得不好，其实相当简单。

1. 单词可以用向量来表示
2. 单词之间的关系可以用向量来表示
3. 关系之间的相似度，可以用向量的余弦定理来表示
4. 于是我们就可以求最相似的词

说到这里，终于能够真正地理解向量和线性代数——刻画多维空间的数学工具。太过精彩。

关于算法细节，这里不再赘述，总之就是余弦定理。

## 应用二：句子情感分类

### 浅层网络

又看起来像一个认知问题，感觉特别难。其实，这次可以换成深度学习（监督学习）的角度。既然每个词的特征都给你提取出来了，那么只需要用一个浅层的神经网路，对这些特征进行一个简单的多分类学习即可！

很简单吧。仔细想想，其实最关键的只有两步。

- word的特征向量
- 机器学习自动求最优解

### 考虑上下文

上述方法的缺点在于，完全没考虑上下文的关系。一个非常直观的例子：

> I am not happy。

这句话明显是表达不开心，但是我们的模型会



